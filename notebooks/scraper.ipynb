{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import requests\n",
    "import tqdm.notebook as tq\n",
    "import random\n",
    "import calendar\n",
    "from datetime import datetime\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_tiktok_gettingstarted():\n",
    "    \"\"\"\n",
    "    Scrape Tiktok Business Center Getting Started Page\n",
    "    \"\"\"   \n",
    "    # Base URL\n",
    "    tt_url=\"https://ads.tiktok.com/help/category?id=5pc1e9Q09towekjAqXm8b1\"\n",
    "    tt_base_url = 'https://ads.tiktok.com/help/'\n",
    "\n",
    "    html = requests.get(tt_url)\n",
    "    bsobj = soup(html.content, 'lxml')\n",
    "\n",
    "    headers = bsobj.findAll(\"a\", {\"class\": \"category_sub_menu_item nuxt-link-active\"})\n",
    "    print(headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_cna(): \n",
    "    \"\"\"\n",
    "    Scrape Channel News Asia Topic Terrorism\n",
    "    \"\"\"   \n",
    "    # Base URL\n",
    "    cna_url=\"https://www.channelnewsasia.com/topic/terrorism\"\n",
    "    cna_base_url = 'https://www.channelnewsasia.com'\n",
    "\n",
    "    # Get webpage\n",
    "    html = requests.get(cna_url)\n",
    "\n",
    "    # Initialise bs object\n",
    "    bsobj = soup(html.content,'lxml')\n",
    "\n",
    "    # Find all headers\n",
    "    headers = bsobj.findAll(\"h6\")\n",
    "\n",
    "    # initialize the progress bar\n",
    "    # select a random color\n",
    "    colors = ['red', 'green', 'yellow', 'blue', 'magenta', 'cyan', 'white', 'steelblue']\n",
    "    random_color = random.choice(colors)\n",
    "    loop = tq.tqdm(enumerate(headers), total=len(headers), \n",
    "                        leave=True, colour=random_color, unit='article')\n",
    "\n",
    "    articles = []\n",
    "\n",
    "    for _, header in loop:\n",
    "        item = {\n",
    "            'headline': header.text\n",
    "        }\n",
    "\n",
    "        # Enter news article\n",
    "        news_link = cna_base_url + header.a['href']\n",
    "        news_html = requests.get(news_link)\n",
    "\n",
    "        # Traverse news article for main body\n",
    "        article = soup(news_html.content,'lxml')\n",
    "        content = article.find('div', {'class': 'content'})\n",
    "        if content:\n",
    "            content_wrapper = content.find_all('div', {'class': 'content-wrapper'})\n",
    "\n",
    "            article_text = []\n",
    "            for content in content_wrapper:\n",
    "                # Extract text\n",
    "                main_text = content.find('div', {'class': 'text'})\n",
    "                if main_text:\n",
    "                    article_text.append(main_text.get_text())\n",
    "        \n",
    "            item['text'] = article_text\n",
    "            articles.append(item)\n",
    "        \n",
    "        # Get timestamp\n",
    "        timestamp_element = article.find('div', {'class': 'article-publish'})\n",
    "        if timestamp_element:\n",
    "            timestamp = timestamp_element.get_text(separator='|', strip=True).split('|')[0]\n",
    "            timestamp_obj = datetime.strptime(timestamp, \"%d %b %Y %I:%M%p\")\n",
    "            item['timestamp_published'] = str(calendar.timegm(timestamp_obj.timetuple()))\n",
    "        \n",
    "        loop.set_postfix(Processing=header.text)\n",
    "        sleep(2)\n",
    "\n",
    "    return articles\n",
    "\n",
    "def format_articles(articles):\n",
    "    \"\"\"\n",
    "    Format scraped data\n",
    "    \"\"\"\n",
    "\n",
    "    formatted_articles = []\n",
    "    \n",
    "    # Format article\n",
    "    for article in articles:\n",
    "        if 'text' not in article:\n",
    "            continue\n",
    "        \n",
    "        article = {\n",
    "            'title': article['headline'],\n",
    "            'body': \"\\n\".join(article['text']),\n",
    "            'timestamp_published': article['timestamp_published'] if 'timestamp_published' in article else 'null'\n",
    "        }\n",
    "\n",
    "        formatted_articles.append(article)\n",
    "\n",
    "    return formatted_articles\n",
    "\n",
    "def scrape_apnews():\n",
    "    \"\"\"\n",
    "    Scrape AP News Topic Terrorism\n",
    "    \"\"\"\n",
    "\n",
    "    # Base URL\n",
    "    cna_url=\"https://apnews.com/hub/terrorism\"\n",
    "\n",
    "    # Get webpage\n",
    "    html = requests.get(cna_url)\n",
    "\n",
    "    # Initialise bs object\n",
    "    bsobj = soup(html.content,'lxml')\n",
    "\n",
    "    # Find all article item in first section\n",
    "    items = bsobj.findAll(\"h3\", {'class':'PagePromo-title'})\n",
    "\n",
    "    # initialize the progress bar\n",
    "    # select a random color\n",
    "    colors = ['red', 'green', 'yellow', 'blue', 'magenta', 'cyan', 'white', 'steelblue']\n",
    "    random_color = random.choice(colors)\n",
    "    loop = tq.tqdm(enumerate(items), total=len(items), \n",
    "                        leave=True, colour=random_color, unit='article')\n",
    "    \n",
    "    articles = []\n",
    "\n",
    "    for _, header in loop:\n",
    "        item = {\n",
    "            'headline': header.text\n",
    "        }\n",
    "\n",
    "        # Enter news article\n",
    "        news_link = header.a['href']\n",
    "        news_html = requests.get(news_link)\n",
    "\n",
    "        # Traverse news article for main body\n",
    "        article = soup(news_html.content,'lxml')\n",
    "        content = article.find('div', {'class': 'RichTextStoryBody'})\n",
    "        if content:\n",
    "            article_texts = content.find_all('p')\n",
    "            article_text = []\n",
    "            for text in article_texts:\n",
    "                article_text.append(text.text)\n",
    "            item['text'] = article_text\n",
    "\n",
    "        timestamp_element = article.find('bsp-timestamp')\n",
    "        if timestamp_element:\n",
    "            item['timestamp_published'] = timestamp_element['data-timestamp']\n",
    "\n",
    "        articles.append(item)\n",
    "        \n",
    "        # Update progress description\n",
    "        loop.set_postfix(Processing=header.text)\n",
    "        sleep(5)\n",
    "    \n",
    "    return articles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
