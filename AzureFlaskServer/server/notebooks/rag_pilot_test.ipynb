{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Displaying final output format\n",
    "from IPython.display import display, Markdown, Latex\n",
    "\n",
    "# Langchain Dependencies\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate, AIMessagePromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langgraph.graph import END, StateGraph\n",
    "# For State Graph \n",
    "from typing_extensions import TypedDict\n",
    "import pandas as pd\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_core.runnables import (\n",
    "    RunnableBranch,\n",
    "    RunnableLambda,\n",
    "    RunnableParallel,\n",
    "    RunnablePassthrough,\n",
    ")\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import Tuple, List, Optional\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from neo4j import GraphDatabase\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores.neo4j_vector import remove_lucene_chars\n",
    "from langchain_core.runnables import ConfigurableField, RunnableParallel, RunnablePassthrough\n",
    "\n",
    "from utils import CSVAgentGPTInstance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "llm_json = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0, model_kwargs={\"response_format\": {\"type\": \"json_object\"}})\n",
    "\n",
    "graph = Neo4jGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The query used a deprecated procedure. ('db.create.setVectorProperty' has been replaced by 'db.create.setNodeVectorProperty')} {position: line: 1, column: 70, offset: 69} for query: \"UNWIND $data AS row MATCH (n:`Document`) WHERE elementId(n) = row.id CALL db.create.setVectorProperty(n, 'embedding', row.embedding) YIELD node RETURN count(*)\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'choice': 'general'}\n"
     ]
    }
   ],
   "source": [
    "vector_index = Neo4jVector.from_existing_graph(\n",
    "    OpenAIEmbeddings(),\n",
    "    search_type=\"hybrid\",\n",
    "    node_label=\"Document\",\n",
    "    text_node_properties=[\"text\"],\n",
    "    embedding_node_property=\"embedding\"\n",
    ")\n",
    "class Entities(BaseModel):\n",
    "    \"\"\"Identifying information about entities.\"\"\"\n",
    "\n",
    "    names: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"All the person, object, location, or event entities that \"\n",
    "        \"appear in the text\",\n",
    "    )\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are extracting person, object, location, or event entities from the text.\",\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Use the given format to extract information from the following \"\n",
    "            \"input: {question}\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "entity_chain = prompt | llm.with_structured_output(Entities)\n",
    "\n",
    "def generate_full_text_query(input: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a full-text search query for a given input string.\n",
    "\n",
    "    This function constructs a query string suitable for a full-text search.\n",
    "    It processes the input string by splitting it into words and appending a\n",
    "    similarity threshold (~2 changed characters) to each word, then combines\n",
    "    them using the AND operator. Useful for mapping entities from user questions\n",
    "    to database values, and allows for some misspelings.\n",
    "    \"\"\"\n",
    "    full_text_query = \"\"\n",
    "    words = [el for el in remove_lucene_chars(input).split() if el]\n",
    "    for word in words[:-1]:\n",
    "        full_text_query += f\" {word}~2 AND\"\n",
    "    full_text_query += f\" {words[-1]}~2\"\n",
    "    return full_text_query.strip()\n",
    "\n",
    "# Fulltext index query\n",
    "def structured_retriever(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Collects the neighborhood of entities mentioned\n",
    "    in the question\n",
    "    \"\"\"\n",
    "    result = \"\"\n",
    "    entities = entity_chain.invoke({\"question\": question})\n",
    "    for entity in entities.names:\n",
    "        response = graph.query(\n",
    "            \"\"\"\n",
    "            CALL db.index.fulltext.queryNodes('entity', $query, {limit: 2})\n",
    "            YIELD node, score\n",
    "            CALL {\n",
    "              WITH node\n",
    "              MATCH (node)-[r:!MENTIONS]->(neighbor)\n",
    "              RETURN node.id + ' - ' + type(r) + ' -> ' + neighbor.id AS output\n",
    "              UNION ALL\n",
    "              WITH node\n",
    "              MATCH (node)<-[r:!MENTIONS]-(neighbor)\n",
    "              RETURN neighbor.id + ' - ' + type(r) + ' -> ' + node.id AS output\n",
    "            }\n",
    "            RETURN output LIMIT 50\n",
    "            \"\"\",\n",
    "            {\"query\": generate_full_text_query(entity)},\n",
    "        )\n",
    "        result += \"\\n\".join([el['output'] for el in response])\n",
    "    return result\n",
    "\n",
    "def retriever(question: str):\n",
    "    structured_data = structured_retriever(question)\n",
    "    unstructured_data = [el.page_content for el in vector_index.similarity_search(question)]\n",
    "    final_data = f\"\"\"Structured data:\n",
    "        {structured_data}\n",
    "        Unstructured data:\n",
    "        {\"#Document \". join(unstructured_data)}\n",
    "    \"\"\"\n",
    "    return final_data\n",
    "\n",
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question,\n",
    "in its original language.\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"  # noqa: E501\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
    "\n",
    "def _format_chat_history(chat_history: List[Tuple[str, str]]) -> List:\n",
    "    buffer = []\n",
    "    for human, ai in chat_history:\n",
    "        buffer.append(HumanMessage(content=human))\n",
    "        buffer.append(AIMessage(content=ai))\n",
    "    return buffer\n",
    "\n",
    "_search_query = RunnableBranch(\n",
    "    # If input includes chat_history, we condense it with the follow-up question\n",
    "    (\n",
    "        RunnableLambda(lambda x: bool(x.get(\"chat_history\"))).with_config(\n",
    "            run_name=\"HasChatHistoryCheck\"\n",
    "        ),  # Condense follow-up question and chat into a standalone_question\n",
    "        RunnablePassthrough.assign(\n",
    "            chat_history=lambda x: _format_chat_history(x[\"chat_history\"])\n",
    "        )\n",
    "        | CONDENSE_QUESTION_PROMPT\n",
    "        | ChatOpenAI(temperature=0)\n",
    "        | StrOutputParser(),\n",
    "    ),\n",
    "    # Else, we have no chat history, so just pass through the question\n",
    "    RunnableLambda(lambda x : x[\"question\"]),\n",
    ")\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Use natural language and be as elaborate as possible.\n",
    "Answer:\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = (\n",
    "    RunnableParallel(\n",
    "        {\n",
    "            \"context\": _search_query | retriever,\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "    )\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "def invoke_chain(question: str, chat_history):\n",
    "    graph.query(\n",
    "        \"CREATE FULLTEXT INDEX entity IF NOT EXISTS FOR (e:__Entity__) ON EACH [e.id]\")\n",
    "    if chat_history:\n",
    "        return chain.invoke(\n",
    "            {\n",
    "                \"question\": question,\n",
    "                \"chat_history\": chat_history\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        return chain.invoke(\n",
    "            {\n",
    "                \"question\": question,\n",
    "            }\n",
    "        )\n",
    "    \n",
    "\n",
    "# Router\n",
    "router_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    You are an expert at routing a user question to either the generation stage or general query stage. \n",
    "    Use the general query for questions that does not require additional contextual information.\n",
    "    Otherwise, you can skip and go straight to the generation phase to respond.\n",
    "    You do not need to be stringent with the keywords in the question related to these topics.\n",
    "    Give a binary choice 'general' or 'generate' based on the question. \n",
    "    Return the JSON with a single key 'choice' with no premable or explanation. \n",
    "    \n",
    "    Question to route: {question} \n",
    "    \n",
    "    \"\"\",\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "\n",
    "# Chain\n",
    "question_router = router_prompt | llm | JsonOutputParser()\n",
    "\n",
    "# Test Run\n",
    "question = \"What's up?\"\n",
    "print(question_router.invoke({\"question\": question}))\n",
    "\n",
    "# General chain\n",
    "general_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    You are an expert at answering general questions. \n",
    "    Use the general query for questions that does not require additional contextual information.\n",
    "    You do not need to be stringent with the keywords in the question related to these topics.\n",
    "    Answer the following question in a natural language. \n",
    "    \n",
    "    Question: {question} \n",
    "    \n",
    "    \"\"\",\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "\n",
    "general_chain = general_prompt | llm | StrOutputParser()\n",
    "\n",
    "  \n",
    "############################################################# Graph State #############################################################\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        search_query: revised question for web search\n",
    "        context: web_search result\n",
    "    \"\"\"\n",
    "    question : str\n",
    "    generation : str\n",
    "    search_query : str\n",
    "    context : str\n",
    "\n",
    "# Node - Generate\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Step: Generating Final Response\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Answer Generation\n",
    "    generation = invoke_chain(question, None)\n",
    "    return {\"generation\": generation}\n",
    "\n",
    "def general(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Step: Generating Final Response\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Answer Generation\n",
    "    generation = general_chain.invoke({\"question\": question})\n",
    "    return {\"generation\": generation}\n",
    "\n",
    "# Conditional Edge, Routing\n",
    "\n",
    "def route_question(state):\n",
    "    \"\"\"\n",
    "    route question to web search or generation.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Step: Routing Query\")\n",
    "    question = state['question']\n",
    "    output = question_router.invoke({\"question\": question})\n",
    "    if output['choice'] == \"general\":\n",
    "        print(\"Step: Routing Query to General\")\n",
    "        return \"general\"\n",
    "    elif output['choice'] == 'generate':\n",
    "        print(\"Step: Routing Query to Generation\")\n",
    "        return \"generate\"\n",
    "    \n",
    "def build_workflow():\n",
    "    \"\"\"\n",
    "    Build the workflow for the graph\n",
    "    \"\"\"\n",
    "    # Build the nodes\n",
    "    workflow = StateGraph(GraphState)\n",
    "    workflow.add_node(\"general\", general)\n",
    "    workflow.add_node(\"generate\", generate)\n",
    "\n",
    "    # Build the edges\n",
    "    workflow.set_conditional_entry_point(\n",
    "        route_question,\n",
    "        {\n",
    "            \"general\": \"general\",\n",
    "            \"generate\": \"generate\",\n",
    "        },\n",
    "    )\n",
    "    workflow.add_edge(\"general\", END)\n",
    "    workflow.add_edge(\"generate\", END)\n",
    "\n",
    "    # Compile the workflow\n",
    "    local_agent = workflow.compile()\n",
    "\n",
    "    return local_agent\n",
    "\n",
    "def run_agent(query, local_agent):\n",
    "    output = local_agent.invoke({\"question\": query})\n",
    "    print(\"=======\")\n",
    "    display(Markdown(output[\"generation\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: Routing Query\n",
      "Step: Routing Query to Generation\n",
      "Step: Generating Final Response\n",
      "=======\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "I'm sorry, but without any specific information or data provided in the context, I am unable to answer the question about how many sales TikTok made yesterday. In order to provide an accurate answer, I would need more details or data related to TikTok's sales from yesterday."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test it out!\n",
    "agent = build_workflow()\n",
    "run_agent(\"How many sales did TikTok make yesterday?\", agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DB Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "llm_json = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0, model_kwargs={\"response_format\": {\"type\": \"json_object\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_list = [\n",
    "  {\n",
    "    \"database_name\": \"Sales_database123\",\n",
    "    \"database_description\": \"A database that contains information about supermarket sales.\",\n",
    "    'columns': \"Invoice ID, Product ID, Shop ID, Customer ID, Date, Time, Payment, Unit Price, Quantity, Total Price\",\n",
    "    \"database_path\": \"./data/sales123_database.csv\",\n",
    "  },\n",
    "  {\n",
    "    \"database_name\": \"Outlet_database\",\n",
    "    \"database_description\": \"A database that contains information about physical outlets.\",\n",
    "    \"columns\":\"Shop ID, Country, City, State, Shop Name, Shop Type, Shop Size, Address\",\n",
    "    \"database_path\": \"./data/sales999_database.csv\",\n",
    "  },\n",
    "  {\n",
    "    \"database_name\": \"Employee_database\",\n",
    "    \"database_description\": \"A database that contains information about employees working at TikTok.\",\n",
    "    \"database_path\": \"./data/employee_database.csv\",\n",
    "  },\n",
    "  {\n",
    "    \"database_name\": \"Product_database\",\n",
    "    \"database_description\": \"A database that contains information about products sold by TikTok.\",\n",
    "    \"database_path\": \"../data/product.csv\",\n",
    "  },\n",
    "  {\n",
    "    \"database_name\": \"Supermarket_Sales_database\",\n",
    "    \"database_description\": \"A database that contains information about supermarket sales.\",\n",
    "    \"database_path\": \"../csv_db/supermarket_sales.csv\",\n",
    "  },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "      (\n",
    "        \"system\",\n",
    "        \"\"\"\n",
    "        Database name: {database_name}\n",
    "        Database description: {database_description}\n",
    "        \"\"\",\n",
    "      ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "  example_prompt=example_prompt,\n",
    "  examples=database_list,\n",
    ")\n",
    "\n",
    "db_router_prompt = ChatPromptTemplate.from_messages(\n",
    "  [\n",
    "    SystemMessagePromptTemplate.from_template(\n",
    "      \"\"\"\n",
    "      You are a expert at routing a user query to one of the database provided below. \n",
    "      Your choice should be one of the database's name. \n",
    "      You can return NA if the query is not related to any of the databases.\n",
    "      Return the JSON with a single key 'choice' with no premable or explanation.\n",
    "\n",
    "      Choices:\n",
    "      \"\"\"\n",
    "    ),\n",
    "    few_shot_prompt,\n",
    "    HumanMessagePromptTemplate.from_template(\n",
    "      \"Query: {query}\"\n",
    "    )\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_router_chain = db_router_prompt | llm | JsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################# Graph State #############################################################\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        query: query\n",
    "        database: database to route query\n",
    "        verbose: print debug statements?\n",
    "        output: LLM generation\n",
    "    \"\"\"\n",
    "    query : str\n",
    "    database : str\n",
    "    output: str\n",
    "    verbose : bool\n",
    "\n",
    "# Node - db_router\n",
    "def db_router_node(state: GraphState):\n",
    "    \"\"\"\n",
    "    dynamic routing to database listed in database_list\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "    if state['verbose']: print(\"Step: Routing Query\")\n",
    "    query = state['query']\n",
    "    output = db_router_chain.invoke({\"query\": query})\n",
    "    if output['choice'] == \"NA\":\n",
    "        if state['verbose']: print(\"Step: Routing Query to General\")\n",
    "        return {'database': \"NA\"}\n",
    "    else:\n",
    "        if state['verbose']: print(\"Step: Routing Query to Database\")\n",
    "        return {'database': output['choice']}\n",
    "    \n",
    "def db_router_edge(state: GraphState):\n",
    "    \"\"\"\n",
    "    dynamic routing to database listed in database_list\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "    if state['database'] == \"NA\":\n",
    "        return \"general\"\n",
    "    else:\n",
    "        return \"db_query\"\n",
    "\n",
    "def database_query(state: GraphState):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    \n",
    "    if state['verbose']: print(\"Step: Generating DB Response\")\n",
    "    query = state[\"query\"]\n",
    "    selected_db = list(filter(lambda db: db.get('database_name') == state['database'], database_list))[0]\n",
    "    path_to_db = selected_db.get('database_path')\n",
    "    csvAgent = CSVAgentGPTInstance()\n",
    "    response = csvAgent.get_csv_agent_output(path_to_db, query)\n",
    "\n",
    "    return {\"output\": response['output']}\n",
    "\n",
    "def general(state: GraphState):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    \n",
    "    if state['verbose']: print(\"Step: Generating General Response\")\n",
    "    question = state[\"query\"]\n",
    "\n",
    "    # Answer Generation\n",
    "    print(f\"Answering general question: {question}\")\n",
    "    return None\n",
    "\n",
    "def build_workflow():\n",
    "    \"\"\"\n",
    "    Build the workflow for the graph\n",
    "    \"\"\"\n",
    "    # Build the nodes\n",
    "    workflow = StateGraph(GraphState)\n",
    "    workflow.add_node(\"db_router_node\", db_router_node)\n",
    "    workflow.add_node(\"general\", general)\n",
    "    workflow.add_node(\"db_query\", database_query)\n",
    "\n",
    "    # Set the entry point\n",
    "    workflow.set_entry_point(\"db_router_node\")\n",
    "\n",
    "    # Build the edges\n",
    "    workflow.add_conditional_edges(\n",
    "        \"db_router_node\",\n",
    "        db_router_edge,\n",
    "        {\n",
    "            \"general\": \"general\",\n",
    "            \"db_query\": \"db_query\",\n",
    "        },\n",
    "    )\n",
    "    workflow.add_edge(\"general\", END)\n",
    "    workflow.add_edge(\"db_query\", END)\n",
    "\n",
    "    # Compile the workflow\n",
    "    \n",
    "    local_agent = workflow.compile()\n",
    "\n",
    "    return local_agent\n",
    "\n",
    "def run_agent(query, local_agent, verbose=True):\n",
    "    output = local_agent.invoke({\"query\": query, \"verbose\": verbose})\n",
    "    print(\"=======\")\n",
    "    if output.get('output'):\n",
    "        display(Markdown(output[\"output\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: Routing Query\n",
      "Step: Routing Query to Database\n",
      "Step: Generating DB Response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The query used a deprecated procedure. ('db.create.setVectorProperty' has been replaced by 'db.create.setNodeVectorProperty')} {position: line: 1, column: 70, offset: 69} for query: \"UNWIND $data AS row MATCH (n:`Document`) WHERE elementId(n) = row.id CALL db.create.setVectorProperty(n, 'embedding', row.embedding) YIELD node RETURN count(*)\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `python_repl_ast` with `{'query': \"df[df['Customer type']=='Member'].groupby('Customer type')['Total'].nlargest(5)\"}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mCustomer type     \n",
      "Member         350    1042.650\n",
      "               557    1034.460\n",
      "               422    1020.705\n",
      "               429     951.825\n",
      "               141     950.250\n",
      "Name: Total, dtype: float64\u001b[0m\u001b[32;1m\u001b[1;3mThe top 5 supermarket sales amounts made by Tristan Tan, who is a member, are as follows:\n",
      "1. $1042.65\n",
      "2. $1034.46\n",
      "3. $1020.71\n",
      "4. $951.83\n",
      "5. $950.25\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "=======\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The top 5 supermarket sales amounts made by Tristan Tan, who is a member, are as follows:\n",
       "1. $1042.65\n",
       "2. $1034.46\n",
       "3. $1020.71\n",
       "4. $951.83\n",
       "5. $950.25"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"Hello, how are you?\"\n",
    "query = \"What are the top 5 supermarket sales amount made by Tristan Tan?\"\n",
    "agent = build_workflow()\n",
    "run_agent(query, agent, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################# Graph State #############################################################\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        query: query\n",
    "        database: database to route query\n",
    "        verbose: print debug statements?\n",
    "        output: LLM generation\n",
    "    \"\"\"\n",
    "    query : str\n",
    "    database : str\n",
    "    output: str\n",
    "    verbose : bool\n",
    "\n",
    "# Node - db_router\n",
    "def db_router_node(state: GraphState):\n",
    "    \"\"\"\n",
    "    dynamic routing to database listed in database_list\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "    if state['verbose']: print(\"Step: Routing Query\")\n",
    "    query = state['query']\n",
    "    output = db_router_chain.invoke({\"query\": query})\n",
    "    if output['choice'] == \"NA\":\n",
    "        if state['verbose']: print(\"Step: Routing Query to General\")\n",
    "        return {'database': \"NA\"}\n",
    "    else:\n",
    "        if state['verbose']: print(\"Step: Routing Query to Database\")\n",
    "        return {'database': output['choice']}\n",
    "    \n",
    "def db_router_edge(state: GraphState):\n",
    "    \"\"\"\n",
    "    dynamic routing to database listed in database_list\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "    if state['database'] == \"NA\":\n",
    "        return \"general\"\n",
    "    else:\n",
    "        return \"db_query\"\n",
    "\n",
    "def database_query(state: GraphState):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    \n",
    "    if state['verbose']: print(\"Step: Generating DB Response\")\n",
    "    query = state[\"query\"]\n",
    "    selected_db = list(filter(lambda db: db.get('database_name') == state['database'], database_list))[0]\n",
    "    path_to_db = selected_db.get('database_path')\n",
    "    csvAgent = CSVAgentGPTInstance()\n",
    "    response = csvAgent.get_csv_agent_output(path_to_db, query)\n",
    "\n",
    "    return {\"output\": response['output']}\n",
    "\n",
    "def general(state: GraphState):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    \n",
    "    if state['verbose']: print(\"Step: Generating General Response\")\n",
    "    question = state[\"query\"]\n",
    "\n",
    "    # Answer Generation\n",
    "    print(f\"Answering general question: {question}\")\n",
    "    return None\n",
    "\n",
    "def build_workflow():\n",
    "    \"\"\"\n",
    "    Build the workflow for the graph\n",
    "    \"\"\"\n",
    "    # Build the nodes\n",
    "    workflow = StateGraph(GraphState)\n",
    "    workflow.add_node(\"db_router_node\", db_router_node)\n",
    "    workflow.add_node(\"general\", general)\n",
    "    workflow.add_node(\"db_query\", database_query)\n",
    "\n",
    "    # Set the entry point\n",
    "    workflow.set_entry_point(\"db_router_node\")\n",
    "\n",
    "    # Build the edges\n",
    "    workflow.add_conditional_edges(\n",
    "        \"db_router_node\",\n",
    "        db_router_edge,\n",
    "        {\n",
    "            \"general\": \"general\",\n",
    "            \"db_query\": \"db_query\",\n",
    "        },\n",
    "    )\n",
    "    workflow.add_edge(\"general\", END)\n",
    "    workflow.add_edge(\"db_query\", END)\n",
    "\n",
    "    # Compile the workflow\n",
    "    \n",
    "    local_agent = workflow.compile()\n",
    "\n",
    "    return local_agent\n",
    "\n",
    "def run_agent(query, local_agent, verbose=True):\n",
    "    output = local_agent.invoke({\"query\": query, \"verbose\": verbose})\n",
    "    print(\"=======\")\n",
    "    if output.get('output'):\n",
    "        display(Markdown(output[\"output\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: Routing Query\n",
      "Step: Routing Query to Database\n",
      "Step: Generating DB Response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The query used a deprecated procedure. ('db.create.setVectorProperty' has been replaced by 'db.create.setNodeVectorProperty')} {position: line: 1, column: 70, offset: 69} for query: \"UNWIND $data AS row MATCH (n:`Document`) WHERE elementId(n) = row.id CALL db.create.setVectorProperty(n, 'embedding', row.embedding) YIELD node RETURN count(*)\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `python_repl_ast` with `{'query': \"df_cash = df[df['Payment'] == 'Cash']\\ntop_5_sales_cash = df_cash.nlargest(5, 'Total')\\ntop_5_sales_cash[['Invoice ID', 'Total']]\"}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m      Invoice ID      Total\n",
      "166  234-65-2137  1003.5900\n",
      "357  554-42-2417  1002.1200\n",
      "141  280-17-4359   950.2500\n",
      "941  702-83-5291   943.2990\n",
      "611  277-35-5865   935.2665\u001b[0m\u001b[32;1m\u001b[1;3mThe top 5 supermarket sales amounts paid using cash are as follows:\n",
      "1. Invoice ID: 234-65-2137, Total: $1003.59\n",
      "2. Invoice ID: 554-42-2417, Total: $1002.12\n",
      "3. Invoice ID: 280-17-4359, Total: $950.25\n",
      "4. Invoice ID: 702-83-5291, Total: $943.30\n",
      "5. Invoice ID: 277-35-5865, Total: $935.27\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "=======\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The top 5 supermarket sales amounts paid using cash are as follows:\n",
       "1. Invoice ID: 234-65-2137, Total: $1003.59\n",
       "2. Invoice ID: 554-42-2417, Total: $1002.12\n",
       "3. Invoice ID: 280-17-4359, Total: $950.25\n",
       "4. Invoice ID: 702-83-5291, Total: $943.30\n",
       "5. Invoice ID: 277-35-5865, Total: $935.27"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"Hello, how are you?\"\n",
    "query = \"What are the top 5 supermarket sales amount paid using cash?\"\n",
    "agent = build_workflow()\n",
    "run_agent(query, agent, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
